Managers Report and Portfolio Analysis 2 Elementary, my dear IBM Watson: Big data and the demise of expertise Last year we discussed how cheap and plentiful bandwidth, together with the adoption of smartphones and tablets and a plethora of new applications had enabled the Internet to become truly ubiquitous, using the post-war experience of the aviation industry as a guide.
This year we turn our attention to a derivative of the mass adoption of IT and one of the hottest topics within technology today, so-called big data a term used to refer to huge datasets created by smartphones and other Internet-connected devices that are too large to manage using traditional solutions.
Just as we have previously used historical experiences of other industries to help us illustrate changes that are unfolding within our sector, this year we reference mining, an industry like IT driven by the need to continually extract value business insight.
While we are not the first to articulate some of these parallels an excellent IBM white paper a few years ago first outlining the analogy with gold mining we are fascinated by how the combination of cheap, scalable computing, together with new tools developed by Internet companies is making it possible to extract value from data sources that just a few years ago would have been uneconomic to data mine.
Just as the ability to process low-grade ore from open pits has transformed the mining industry, so big data and the potential to automate knowledge work is likely to transform the economics of many industriesover the coming years.
Every two days we generate 5 quintillion Machine-generated data such as web bytes of data about the same amount it logs, call records, financial trades, 5.1bn took from the dawn of civilization until network events and sensor readings Daily searches 2003 to create.
Growth has been so is likely to create even larger datasets on Google dramatic that 90% of the data in the world as the number of networked devices is today is said to have been created in the forecast to be twice the size of the global last two years alone.
In addition to its population by 2015.
This so-called unprecedented volume, big data is also Internet of Things IoT is being driven defined by its variety both structured by the use of sensors, actuators and and unstructured, velocity real-time cheap connectivity with more than Data growth has been analysis and veracity the need to 30bn RFID radio frequency identification substantially driven establish trust in the data.
Growth has tags in circulation at the end of 2011. by 2.4bn global been substantially driven by 2.4bn global While the IoT is today best understood Internet users and Internet users and 1.5bn smartphone as a collection of interesting niche 1.5bn smartphone subscribers that between them undertake applications, the GE vision of an Industrial subscribers 5.1bn daily searches on Google, watch Internet does not seem like a distant one.
6bn hours of video every month on The prospect of extracting value from YouTube and receive 145bn emails every these untapped sources has led data day.
Social media has also played a very The prospect of to be described as the next natural significant role, evidenced by 12bn extracting value from resource while others have gone terabytes of daily tweets and 300m photos these untapped sources further in declaring it to be a new class of uploaded daily to Facebook, while on has led data to be economic asset, like currency or gold.
New Years Eve, mobile messaging app described as the next WhatsApp delivered 18bn messages.
natural resource Polar Capital Technology Trust plc Annual Report & Financial Statements for the year ended 30 April 2013 23 Managers Report and Portfolio Analysis 2 Elementary, my dear IBM Watson: Big data and the demise of expertise continued Data scientists a term coined by the database, but per byte it is also C. 5% head of analytics at LinkedIn in 2008 to intrinsically far less valuable which describe the analysts that prepare, crunch makes it near impossible to justify the Estimated amount and interpret data are in short supply, cost of storing vast quantities in an of data retained by while a recent Harvard Business Review expensive relational database or enterprises today article titled, Data scientist the sexiest enterprise data warehouse.
Instead job of the 21st century captured the of storing everything a necessary zeitgeist perfectly.
pre-condition for big data to prove transformative enterprises are While there is little new about the retaining only the data they perceive concept of data mining a term that to be of sufficient value estimated has been in circulation since the early at just C. 5% of the total while discarding 1990s, the advent of cheap storage and the rest.
The need for new solutions able massively parallel computing has made to cheaply process large volumes of it possible to consider what we might During its short life the low-grade data is highly reminiscent unearth now that we are able to ask IT industry has been of the most significant challenge faced questions of the 99% of the worlds almost exclusively by the mining industry.
data that isnt analysed today.
However, concerned with during its short life the IT industry has The mining industry knows all too capturing, storing and been almost exclusively concerned well about having to retool in order to processing high-value with capturing, storing and processing continue to extract value from lower structured data high-value structured data such as quality sources.
At the time of the sales orders, employee information or California gold rush which began when inventory data.
Structured data growth John Sutter discovered gold at his since the 1970s fuelled by the shift sawmill in 1848 the alluvial gold from paper to digital records saw deposited in riverbeds was so richly the birth of the relational database concentrated that the so-called fortyThe advent of big data pioneered by Oracle and IBM, database niners miners who had arrived in has left existing tool companies such as Computer California during 1849 simply panned solutions looking Associates and BMC Software and some for gold.
However, by 1850 most of the somewhat anachronistic years later, business analytics vendors easily accessible gold had been mined now that 80% of the including Cognos and Business Objects leaving deposits more closely resembling worlds data is said that enabled their customers to analyse the wider mining industry that was to be unstructured or at least generate reports on this already engaged in a continuous battle to structured data.
extract value from ever diminishing ore grades.
Today gold ore in Australia and However the advent of big data has South Africa is just 10% and 25% as rich left these existing solutions looking respectively as it was a century ago with somewhat anachronistic now that ongoing gradual declines anticipated in It is rarer today to 80% of the worlds data is said to be most metals and minerals.
To give some find a one-ounce unstructured.
Not only does this data perspective on how significantly ore nugget of gold than not lend itself to being stored in a rigid grades have diminished, it is rarer today a five-caret diamond Polar Capital Technology Trust plc 24 Annual Report & Financial Statements for the year ended 30 April 2013 The need to counter lower grade and more intractable ore has driven technology progress in both mining and smelting throughout the ages to find a one-ounce nugget of gold than a economics of the silver industry by 6bn five-carat diamond.
The need to counter working on low-grade ore and tailings lower grade and more intractable ore that had previously been considered waste.
Tones of rock has driven technology progress in both The next major advancement came in the extracted to date from mining and smelting throughout the ages, late 19th century using water and oil to the Kennecott mine generating demand for cheaper inputs, separate sulphide minerals from waste improved capital equipment and new rock.
Known as flotation, this technique metallurgical processes.
allowed Broken Hill the BH in BHP Billiton to become one of the worlds While the history of mining technology is major sources of zinc.
However, it was well beyond the remit of this paper, the the development of carbon-in-pulp heap most critical advances have focused on leaching that used cyanide to dissolve seeking out richer ore grades and metallic gold that revolutionised the extending mine life by allowing shafts to mining industry in the early 20th century be sunk deeper, improving the rate of by making it economical to mine gold at extraction and converting more mined rock just 0.75 ppm parts per million facilitating into metal.
In the case of deeper mines, the the development of large, low-grade use of shafts and tunnels was greatly deposits through open-cut mining.
enhanced by lifts, improved ventilation and the use of explosives that had become The first open pit mine established at commonplace by the end of the 17th Bingham Canyon, Utah in 1906 to exploit century.
Unfortunately, as shafts sank copper captivated the mining worldby further into the earth and discovered demonstrating how mass production subterranean streams, managing water techniques could extract value from rock At 2 miles wide and became more critical leading to the use of that would previously not have attracted a more than of a mile vacuum pumps that replaced buckets and second glance.
Since then, the Kennecott deep, the Kennecott chains previously used to recover water Copper Mine as it is now known has open mine is one of from mine shafts.
However, as mines become the worlds largest open pit mine only two man-made extended further below ground so new yielding more than 18.1m tons of copper objects distinctly problems were encountered in the form from 6bn tons of rock extracted to date.
visible from space of greater travel time in the 18th Six billion tons of rock represents a scale century it might take a miner two difficult to convey but at 2 miles wide and hours to reach the face.
more than of a mile deep, the Kennecott open mine once the cornerstone of the Widespread adoption Worse still, ores became increasingly Guggenheim empire is one of only two of open pit mining intractable necessitating new man-made objects distinctly visible from required a significant metallurgical processes to justify space.
Widespread adoption of open pit re-tooling of the industry underground extraction.
While liquation mining required a significant re-tooling of as focus shifted from had helped liberate silver from copper by the industry as focus shifted from targeting targeting richer ores in first combining it with lead as early as richer ores in favour of processing vast favour of processing 1453, mercury amalgamation developed quantities of lower grades.
vast quantities of a century later changed the entire lower grades Polar Capital Technology Trust plc Annual Report & Financial Statements for the year ended 30 April 2013 25 New Tools ITS HAPPENING AGAIN.
Cheap, scalable and schema-later distributed file systems and NoSQL databases have significantly reduced the upfront costs of data storage, just as open pit mining dispensed with the need to dig shafts and tunnels before extraction could commence.
The need to counter lower grade and more Dramatic cost declines in storage, compute and networking intractable ore has driven technology progress in has made it economically viable to store information that both mining and smelting throughout the ages.
Prior would have been previously discarded or ignored.
Internet to the 20th century new tools had been focused on seeking companies such as Google, Yahoo and Facebook have also out richer ore grades at deeper depths, driving advances played a critical role in pioneering open-source tools such in ventilation, vacuum pumps and lifts to name a few.
as Hadoop that make it possible to economically process However, the advent of open-pit mining required large volumes of low-grade data.
a significant re-tooling of the industry as focus shifted from targeting richer ores in favour of processing vast C. 80% quantities of lower grades.
This has been a boon for Amount of the worlds data believed to be unstructured capital equipment providers such as Caterpillar, Komatsu and Terex who supply the electric shovels, wheel excavators and haul trucks because open pits produce $16.9bn 810x as much waste rock as underground mines.
Estimated value of the big data market by 2015 Photo: Truck being loaded with ore P Po ol la ar C r Ca ap pi it ta al T l Te ec ch hn no ol lo og gy T y Tr ru us st p t pl lc c 26 26 A An nn nu ua al R l Re ep po or rt & F t & Fiin na an nc ciia al S l St ta at te em me en nt ts f s fo or t r th he y e ye ea ar e r en nd fide ed 3 d 30 A 0 Ap pr riil 2 l 20 01 13 3 Managers Report and Portfolio Analysis 2 Elementary, my dear IBM Watson: Big data and the demise of expertise continued This has been a boon for the likes of databases that require data to be indexed 4.3bn Bucyrus, Caterpillar and Terex because on ingestion, NoSQL alternatives allow open pits produce 810x as much waste data to be stored as is, allowing schema Daily number of data rock as underground mines and therefore to be applied at the point of request base writes at Netflix require significantly larger scale capital which significantly reduces front-end equipment.
As a result, haul truck costs, just as open pit mining dispensed payloads have increased from 200t in with the need to dig shafts and tunnels 1990 to almost 350t today while the bucket akin to columns and rows in a database volume of wheel excavators has increased before extraction could commence.
3 3 from 25m to 40m over the past five years However, the best example of the role alone.
While Shakespeare once warned played by Internet companies is Hadoop, that all that glitters is not gold, it may also an open source framework for now be true that all that is gold does not distributed file system processing.
glitter given that two-thirds of current Originally spawned by Yahoo who were The ability to extract production is mined in open-pits at struggling with Internet-scale data value from low-grade 12ppm, gold only being visible to the processing on large centralised servers ore is remarkedly eye at 30ppm!
and named after creator Doug Cuttings analogous to the sons stuffed toy elephant, Hadoop This ability to extract value from lowpromise of big data allows commodity hardware a node grade ore is remarkedly analogous to that when datasets to be unified into a single cluster the promise of big data that when are huge, even low enabling what is known as massively datasets are huge, even low quality quality bytes are likely parallel computing.
bytes are likely to prove valuable.
This to prove valuable observation would be an academic one This new software open sourced and but for the dramatic cost declines in able to run on commodity hardware storage, compute and networking that has drastically reduced the cost of Internet companies have made it economically viable to store storing and managing data making it have pioneered new information that would have previously economically viable to retain and process technologies able to been discarded or ignored, just as low-grade data.
In addition like larger handle massive datasets cyanide heap leaching made open-pit haul trucks and steam shovels these mining a possibility.
Equally as critical new tools have been designed for has been the work of large Internet dramatically greater volumes, evidenced companies who pioneered new by Dremel, Googles ad-hoc query technologies able to handle massive system for structured data that can run datasets that were overwhelming queries over trillion-row tables in The best example of traditional approaches.
This work has seconds and scales to thousands of the role played by significantly advanced the cause of CPUs and petabytes of data.
Wrestling Internet companies open-source NoSQL databases such as with some of the largest datasets and is Hadoop, an open HBase modelled on Googles Bigtable, reticent to discard any data, Internet source framework its proprietary data storage system and companies have naturally been some for distributed file Cassandra originally created by of the earliest and most significant system processing Facebook.
Unlike traditional relational adopters of these new technologies.
Polar Capital Technology Trust plc Annual Report & Financial Statements for the year ended 30 April 2013 27 Managers Report and Portfolio Analysis 2 Elementary, my dear IBM Watson: Big data and the demise of expertise continued The adoption of NoSQL databases is resist the vastly lower costs associated 2tr already widespread within this with these emerging solutions, estimated constituency, Cassandra is being used to be 8590% cheaper than traditional Number of objects by Netflix to store 95% of its data, ones from the likes of Oracle.
stored on Amazons S3 processing on average 2.1bn reads and Just as new metallurgical processes 4.3bn writes to their databases every transformed what was once considered day while music streaming service waste rock into a valuable and plentiful Spotify uses it to manage more than one source, so significantly lower costs billion playlists.
Hadoop described by associated with these new software tools eBay as an amazing technology stack are making it possible for companies to has also been widely adopted by store everything in a distributed file system Internet companies with Yahoo reported such as Hadoop, rather than discard it as in to have over 40,000 nodes storing more the past.
For example, US retailer Sears than 40 petabytes of data.
While Twitter used to retain their data for between 90 As early big data uses MySQL a more traditional, days and two years but having adopted successes by Internet relational database to store its tweets, it Hadoop keep everything.
In addition, companies become also utilises both Cassandra and Hadoop companies such as Amazon Web Services better known and the open-source library Lucene to AWS now offer hosted, pay-as-you-go enterprises are unlikely process 2.1bn daily search queries.
versions of a number of these new to be able to resist the Although it doesnt disclose which big solutions including Elastic Map Reduce vastly lower costs data enabling technologies it employs, a Hadoop framework and Redshift, a associated with these Amazon S3 its Simple Storage Service cloud-based data warehouse DW said emerging solutions stores a remarkable 2trillion objects to be a tenth of the cost of a traditional today while handling peak requests of DW.
The ability to access these new tools more than 1.1m per second.
as a service is likely to accelerate both In contrast to Internet companies with a enterprise adoption and the development pressing need for new tools to manage of new applications that combine cloud US retailer Sears used vast quantities of low quality data, and big data technologies, such as the to retain their data for enterprises have been relatively slow to platform by Illumina that leverages AWS between 90 days and embrace big data largely because their infrastructure to help customers extract two years but having traditional focus on high value structured value from the massive datasets created adopted Hadoop data has left them looking a little like by their DNA sequencing tools.
keep everything underground miners at the turn of the As they become more widespread, we 20th century bristling with overconcur with Forresters assessment that engineered tools designed for mining rich big data solutions will allow companies seams just at the point when low-grade to deploy predictive models to improve The ability to access open pit mining was about to take off.
business performance or mitigate risk.
these new tools as However, as early big data successes Likewise we agree with IBM that a service is likely by Internet companies become better organisations that can run analytics to accelerate known and newer technologies mature, against their entire data sets definitively enterprise adoption enterprises are unlikely to be able to have an advantage over those that do Polar Capital Technology Trust plc 28 Annual Report & Financial Statements for the year ended 30 April 2013 As the mining experience demonstrates, massive datasets fundamentally challenge our basic assumptions about the value of data sensors, RFID tags and security not.
However, neither statement comes 500m systems.
This enables real-time visibility close to capturing our enthusiasm for into what is happening across a firms IT big data, which is driven instead by what Number of systems and infrastructure, what Splunk we believe becomes possible once daily tweets calls operational intelligence.
Palantir analysis can be applied to population, a private company whose software is rather than sample data.
As the mining experience demonstrates, massive based on technology developed by datasets fundamentally challenge our PayPal to detect fraud delivers basic assumptions about the value of intelligence augmentation allowing data because when n the sample size analysts to quickly explore data is exceptionally large there will be from multiple sources.
enormous value to be extracted from the According to reports, Palantir is used by dataset even where the individual data the US military to better predict the points or bytes appear worthless.
A good location of improvised explosive devices example of this phenomenon is Twitter IEDs in Afghanistan and was a major where the value of a small number of factor in the successful raid on Osama tweets is essentially zero but when n bin Laden in May 2011.
Twitter is using 500m the number of daily tweets the the open-source event-processing system value of this vast dataset is enormous, Storm to extract emerging trends from evidenced by the $10bn valuation tweets in real-time so that when a story currently being applied to the company.
begins to emerge, it will be identified by Likewise, we are unlikely to be able to its trending topics algorithm.
Similarly extract any value from a handful of till the remarkable number of search queries receipts, but with billions we could allows Google to predict regional create the worlds best recommendation outbreaks of flu up to ten days earlier engine.
However, Google represents the Big data tools are than they are reported by the Centres best example of the value that can be making it possible to of Disease Control and Prevention while derived from analysing population data provide near-real time Waze, a community-driven navigation because while the value of indexing one analysis, often across app provides real-time routing and traffic webpage is negligible, cataloguing 46bn multiple datasets updates by monitoring users journey webpages helped the company generate times.
In addition, visualisation tools from $40bn in revenues last year.
the likes of Qlik Technology and Tableau The remarkable number In addition to being able to apply should help identify aberrations earlier, of search queries allows analytics to population data, big data which could have profound implications Google to predict tools are making it possible to provide for fraud detection and medical diagnosis.
regional outbreaks of near-real time analysis, often across Being able to inexpensively question flu up to ten days earlier multiple datasets.
Companies such as and cross-reference massive datasets than they are reported Splunk are helping their 4,800 customers of varying data types should also by the Centres of to collect, monitor, index and analyse result in previously unknown Disease Control machine data generated by IT relationships being discovered.
and Prevention infrastructure servers, routers Polar Capital Technology Trust plc Annual Report & Financial Statements for the year ended 30 April 2013 29 Managers Report and Portfolio Analysis 2 Elementary, my dear IBM Watson: Big data and the demise of expertise continued The University of Ontario Institute of Over time, the decisions and predictions $60bn Technology is helping a Toronto hospital to made possible by big data are likely to monitor the health of babies by collecting prove superior to those traditionally made Estimated more than 1,000 pieces of diagnostic by humans reliant on using sample sets, annual cost of US information per second in order to detect probability theory and gut feel.
Just as the healthcare fraud life-threatening infections a day earlier Internet has arguably diminished the value than previously possible by crossof knowledge, so big data has the referencing multiple data streams that potential to reduce the value of decisionare operating within normal parameters making and expertise.
For instance, the instead of just looking for extreme age old skill of being able to recognise a readings or outliers.
Big Data is also tune from just a few notes has been likely to be used by doctors to predict eliminated by Shazam, a mobile app that outcomes particularly now that the can identify any song by creating a unique Affordable Care Act has shifted pay from signature that is cross-referenced against service to outcomes.
It will also likely its database of 11m songs.
The app is used transform the way that genomic data is by 250m people using smartphones to used in research and clinical applications, recognise three billion songs annually particularly now that millions of DNA base while artists such as Lady GaGa uses the pairs can be sequenced in a very short data generated by apps like Shazam and time using highly parallel systems on a Spotify to help tailor tour routes and live $5000 machine that ten years ago cost sets for specific shows.
Zillow is an online nearly $3bn.
real estate company that is able to produce Lady GaGa uses the its own estimate of a homes value a The ability to cross-reference patient data generated by zestimate by combining user submitted data, curated content and previous clinical apps like Shazam and data with its own database of 110m US trials should also make it significantly Spotify to help tailor homes.
PRO Holdings uses massive easier to identify biomarkers.
Drug tour routes and live datasets to help its customers optimise discovery and biotechnology research sets for specific shows pricing across a range of industries.
could be accelerated while personalised Amazon uses algorithms to heavily medicine could finally become a reality.
In customise the browsing experience for the marketing domain, the ability to ask returning customers while Netflix questions of aggregated anonymous analysed the viewing habits of its 33m Facebook data will allow would-be users to correctly predict the success of advertisers to discover previously The most remarkable its hit TV series House of Cards, undiscovered relationships to exploit.
example of diminution commissioning the $100m project without Being able to cross reference separate of expertise is provided seeing one scene first.
However, the most databases should also help identify by IBMs Watson, a remarkable example of diminution of relationships that should not exist which supercomputer that in expertise is provided by IBMs Watson, a will be of particular interest to law 2011 appeared on US supercomputer that in 2011 appeared on enforcement and intelligence agencies game show Jeopardy!, US game show Jeopardy!, beating two while helping to reduce fraud that in beating two former former champions.
Watson is comprised of healthcare alone is said to cost US champions a cluster of ninety IBM Power 750 servers taxpayers $60bn a year.
Polar Capital Technology Trust plc 30 Annual Report & Financial Statements for the year ended 30 April 2013 Watson is now being used by oncologists in New York to speed up cancer diagnosis with 2880 POWER7 processor cores while operations today account for 57% of all 1m sec its 16 TBs of memory allow it to store 200m mines and 85% of total tonneage.
Worse pages of content and 500 gigabytes of still, the experience likely to be felt by Information pre-processed information.
Able to existing technology vendors will prove equivalent to understand natural language and learn more deleterious than in mining where 1m books from its success and failure, Watsons higher prices benefited all suppliers, even processed every use of Hadoop allowed it to process those unable to benefit from greater second by Watson information equivalent to one million books volumes.
In contrast, new tools designed every second.
Having won easily, Watson is to manage low-grade data do not cost now being used by oncologists in New York millions of dollars like the latest electric to analyse 2m pages of medical journals shovels and haul trucks but are largely and 600k pieces of medical evidence in open-sourced and able to utilise order to speed up cancer diagnosis.
As such they are likely to exert significant downward While the current consensus holds pressure on pricing of existing solutions that Hadoop and other open-source that look increasingly anachronistic.
Just alternatives will co-exist with traditional as towns and cities created during the 19th technologies, the reality is that existing century gold rushes attempted to reinvent databases, data warehouses, business themselves with varying success once intelligence and tools have been designed the richest ore had been depleted, so for structured data that now accounts for incumbent vendors are likely to use M&A only C. 20% of the data in the world.
With to remain pertinent, evidenced by IBM the vast majority of incremental data Just as towns and spending $16bn on 35 acquisitions to boost growth coming from lower grade sources, cities created during its analytics capabilities since 2005. focus will likely shift further from existing the 19th century gold technologies for instance, the ability to Of course there are a number of risks rushes attempted to store everything in a distributed file that could prevent our vision of big reinvent themselves system is likely to pressure ETL extract, data from transpiring.
Today the most so incumbent vendors transform and load vendors such as significant one relates to privacy, an issue are likely to use M&A Informatica who help create subsets of that recently returned to prominence to remain pertinent a database to load into a traditional data following leaks that revealed how the warehouse.
Of course, high value National Security Agency NSA the US transactional data OLTP will remain in intelligence agency was intercepting the relational databases and enterprise data use of major US websites by foreigners warehouses, just as a significant number and analysing metadata contained in While the use of data of applications still utilise mainframes millions of US phone calls.
While the mining will inevitably and high-end UNIX servers.
use of data mining by security or law raise privacy concerns, enforcement agencies will inevitably they are best understood But this co-existence is likely to prove raise privacy and legality concerns, as part of the wellone-sided with most of the growth likely they are best understood as part of understood trade off to be captured by the newer, disruptive the well-understood trade off between between civil liberties technologies just as open pit and placer civil liberties and national security.
and national security Polar Capital Technology Trust plc Annual Report & Financial Statements for the year ended 30 April 2013 31 Managers Report and Portfolio Analysis 2 Elementary, my dear IBM Watson: Big data and the demise of expertise continued The broader debate about privacy also manage and process vast quantities of 90% appears a little dishonest because low-grade data have made it possible to consumers bemoan their loss of privacy apply analytics to datasets that just a few of the worlds gold but rarely complain when the same tools years ago would have been discarded or has been mined since are used to deliver something they want, overlooked by all but the largest Internet the end of the evidenced by their love-hate relationship companies.
Given that 90% of the worlds California gold rush with Facebook criticised for its apparent gold has been mined since the end of the disregard for privacy and yet able to boast California gold rush, despite average ore 1.1bn monthly active users!
This grades that are C. 20% of where they stood inconsistent attitude is reminiscent of the at that time the outlook for big data looks dislike of surveillance cameras that assured.
Of course there are a number of capture more than 150 images of the potential risks to this view, the most average London commuter every day but significant being related to privacy, have helped to reduce crime to the lowest regulation and in the future the ethics There are some real levels for thirty years.
That said, there are of predictive analytics.
However, we are data issues that will some real data issues that will need to be hopeful that none of these issues will need to be tackled tackled over the coming years relating to prove insurmountable.
over the coming years its ownership, retention, resale, sharing While futurist visions of autonomous and sanctity.
While more transparency, machine learning and artificial intelligence together with greater use of anonymised remain very much in their infancy, IBM aggregated data should ameliorate the Watson is an extraordinary example of issue, greater regulation appears As our ability to what is possible when massive datasets inevitable.
For now this is unlikely to prove extract insight from can be analysed in real time.
As our ability too proscriptive although it might require big data continues to to extract insight from big data continues a period of adjustment as experienced by improve so it is likely to improve so it is likely to transform a the mining industry once it became to transform industries number of industries including healthcare subject to greater environmental such as healthcare and marketing as it becomes possible to scrutiny.
Greater regulation may also and marketing automate decision-making and reduce prove beneficial by promoting the cause of wasted expenditure.
With the market security, necessary to protect data from forecast to be worth $16.9bn in 2015 a the rising threat of state-sponsored fivefold increase since 2010 big data cyber-crime that closely resembles 16th represents exactly what new technology century privateers such as Sir Francis Big data represents cycles are about: collapsing computing Drake who intercepted treasure fleets exactly what new costs that make new application sets from the Spanish Main.
technology cycles possible that will change business The combination of open pits and improved are about: collapsing processes forever.
metallurgical processes such as cyanide computing costs that Ben Rogoff leaching transformed mining by making it make new application possible to extract value from low cost ore sets possible that that was previously considered unviable.
will change business Likewise, new tools designed to store, processes forever Polar Capital Technology Trust plc 32 Annual Report & Financial Statements for the year ended 30 April 2013 Extracting Value ITS HAPPENING AGAIN.
Just as metallurgical advances and alchemists made it possible to mine ore and tailings once considered waste, so open-source technologies and data scientists are enabling value to be extracted from data that may have previously been discarded.
Metallurgical advances have been critical in ameliorating The prospect of extracting value from new, untapped the impact of falling ore grades by transforming what was sources has led data to be described as the next natural once considered waste rock into a valuable and plentiful resource and likened to currency or gold.
This process began in the mid 15th century with pioneered by Internet companies such as Google have been liquation, followed by mercury amalgamation a hundred critical, so-called data scientists the analysts who years later.
More recently, the advent of heap leaching prepare, crunch and interpret data are best understood the use of cyanide to dissolve metallic gold has as the alchemists and master miners of the big data era.
revolutionised the industry by making it economically possible to mine gold at just 0.75 ppm parts per million 1bn facilitating the development of open-pit mining.
However, number of playlists managed by Spotify using these metallurgical breakthroughs relied heavily on the Cassandra database specialists such as Georgius Agricola known as the father of mineralogy whose 16th century treatise fide Re Metallica remained the standard reference work on 200m mining, extraction and metallurgy for two hundred years.
The number of pages of information that IBM Watson can process in one second Photo: Gold Smelting Process P Po ol la ar C r Ca ap pi it ta al T l Te ec ch hn no ol lo og gy T y Tr ru us st p t pl lc c A An nn nu ua al R l Re ep po or rt & F t & Fiin na an nc ciia al S l St ta at te em me en nt ts f s fo or t r th he y e ye ea ar e r en nd fide ed 3 d 30 A 0 Ap pr riil 2 l 20 01 13 3 33 33
